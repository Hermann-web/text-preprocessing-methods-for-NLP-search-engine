{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "search_engine_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jpw-Vq31sH4"
      },
      "source": [
        "# **1: Créer les keywords à partir d'une phrase en se basant sur les mots d'un dictionnaire et un corpus de texte en passant par la tokenization, la correction, la lemmatization et le removeStopWords**\n",
        "---\n",
        "*importance: yes run it*\n",
        "---\n",
        "`fonction: SENTENCE_TO_CORRECT_WORDS`\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNdDV6t8KlUl",
        "outputId": "65a5da94-61e4-4088-cb66-04d9ca57d258"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import unicodedata, re, string\n",
        "import json \n",
        "\n",
        "\n",
        "def get_dico():\n",
        "    textdir = \"liste.de.mots.francais.frgut_.txt\"\n",
        "    try:DICO = open(textdir,'r',encoding=\"utf-8\").read()\n",
        "    except: DICO = open(textdir,'r').read()\n",
        "    \n",
        "    textdir = 'corpus_.txt'\n",
        "    try:CORPUS = open(textdir,'r',encoding=\"utf-8\").read();found=True\n",
        "    except:pass\n",
        "    try: CORPUS = open(textdir,'r').read();found=True \n",
        "    except: pass\n",
        "    CORPUS = open(textdir,'r',encoding='cp1252').read();found=True \n",
        "\n",
        "    \n",
        "    \n",
        "    #WORDS = Counter(words( 'manger bouger difference update All edits that are one edit away from `word`. The subset of `words` that appear in the dictionary of WORDS '))\n",
        "    \n",
        "    return DICO+CORPUS\n",
        "\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    '''\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii\n",
        "    '''\n",
        "    \"\"\"This method removes all diacritic marks from the given string\"\"\"\n",
        "    norm_txt = unicodedata.normalize('NFD', input_str)\n",
        "    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n",
        "    return unicodedata.normalize('NFC', shaved)\n",
        "\n",
        "def clean_sentence(texte):\n",
        "    # Replace diacritics\n",
        "    texte = remove_accents(texte)\n",
        "    # Lowercase the document\n",
        "    texte = texte.lower()\n",
        "    # Remove Mentions\n",
        "    texte = re.sub(r'@\\w+', '', texte)\n",
        "    # Remove punctuations\n",
        "    texte = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', texte)\n",
        "    # Remove the doubled space\n",
        "    texte = re.sub(r'\\s{2,}', ' ', texte)\n",
        "    #remove whitespaces at the beginning and the end\n",
        "    texte = texte.strip()\n",
        "    \n",
        "    return texte\n",
        "\n",
        "'''\n",
        "#pas cool car il ya des nombres importants\n",
        "def tokenize_sentence3(texte):\n",
        "    #retourner les groupes d'alphabets\n",
        "    return re.findall(r'\\w+', texte.lower())\n",
        "'''\n",
        "'''\n",
        "#inutile ici car sa VA est qu'ik decoupe en phrases\n",
        "def tokenize_sentence2(texte):\n",
        "        #clean the sentence\n",
        "    blob_object = TextBlob(texte)\n",
        "        #tokenize\n",
        "    liste_words = blob_object.words\n",
        "        #return \n",
        "    return liste_words\n",
        "'''\n",
        "def tokenize_sentence(texte):\n",
        "        #clean the sentence \n",
        "    texte = clean_sentence(texte)\n",
        "        #tokenize \n",
        "    liste_words = texte.split()\n",
        "        #return \n",
        "    return liste_words\n",
        "\n",
        "def strip_apostrophe(liste_words):\n",
        "    get_radical = lambda word: word.split('\\'')[-1]\n",
        "    return list(map(get_radical,liste_words))\n",
        "\n",
        "def pre_process(sentence):\n",
        "    #remove '_' from the sentence \n",
        "    sentence = sentence.replace('_','')\n",
        "    \n",
        "    #get words fro the sentence \n",
        "    liste_words = tokenize_sentence(sentence)\n",
        "    #cut out 1 or 2 letters ones \n",
        "    liste_words = [elt for elt in liste_words if len(elt)>2]\n",
        "    #prendre le radical après l'apostrophe\n",
        "    liste_words = strip_apostrophe(liste_words)\n",
        "    print('\\nsentence to words : ',liste_words)\n",
        "    return liste_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "    \n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def DICO_ET_CORRECTEUR():\n",
        "    \"cette fonction retourne la liste des mots de dictionnaire\"\n",
        "    DICO = get_dico()\n",
        "    WORDS = Counter(pre_process(DICO)) #Counter prends un str et retourne une sorte de liste enrichie\n",
        "    \"correction des mots \"\n",
        "    N = sum(WORDS.values())\n",
        "    P = lambda word: WORDS[word] / N #\"Probability of `word`.\"\n",
        "    \n",
        "    correction = lambda word: max(candidates(word), key=P) #\"Most probable\n",
        "    return WORDS,correction\n",
        "\n",
        "WORDS,CORRECTION = DICO_ET_CORRECTEUR()\n",
        "\n",
        "#//https://www.ranks.nl/stopwords/french\n",
        "#STOPWORDS =['alors', 'au', 'aucun', 'aussi', 'autre', 'avant', 'avec', 'avoir', 'bon', 'car', 'ce', 'cela', 'ces', 'ceux', 'chaque', 'ci', 'comme', 'comment', 'dans', 'des', 'du', 'dedans', 'dehors', 'depuis', 'devrait', 'doit', 'donc', 'dos', 'début', 'elle', 'elles', 'en', 'encore', 'essai', 'est', 'et', 'eu', 'fait', 'faites', 'fois', 'font', 'hors', 'ici', 'il', 'ils', 'je\\tjuste', 'la', 'le', 'les', 'leur', 'là', 'ma', 'maintenant', 'mais', 'mes', 'mien', 'moins', 'mon', 'mot', 'même', 'ni', 'nommés', 'notre', 'nous', 'ou', 'où', 'par', 'parce', 'pas', 'peut', 'peu', 'plupart', 'pour', 'pourquoi', 'quand', 'que', 'quel', 'quelle', 'quelles', 'quels', 'qui', 'sa', 'sans', 'ses', 'seulement', 'si', 'sien', 'son', 'sont', 'sous', 'soyez\\tsujet', 'sur', 'ta', 'tandis', 'tellement', 'tels', 'tes', 'ton', 'tous', 'tout', 'trop', 'très', 'tu', 'voient', 'vont', 'votre', 'vous', 'vu', 'ça', 'étaient', 'état', 'étions', 'été', 'être']\n",
        "STOPWORDS = [\"a\",\"à\",\"â\",\"abord\",\"afin\",\"ah\",\"ai\",\"aie\",\"ainsi\",\"allaient\",\"allo\",\"allô\",\"allons\",\"après\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aujourd\",\"aujourd'hui\",\"auquel\",\"aura\",\"auront\",\"aussi\",\"autre\",\"autres\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avoir\",\"ayant\",\"b\",\"bah\",\"beaucoup\",\"bien\",\"bigre\",\"boum\",\"bravo\",\"brrr\",\"c\",\"ça\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceux-là\",\"chacun\",\"chaque\",\"cher\",\"chère\",\"chères\",\"chers\",\"chez\",\"chiche\",\"chut\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"delà\",\"depuis\",\"derrière\",\"des\",\"dès\",\"désormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dixième\",\"dix-neuf\",\"dix-sept\",\"doit\",\"doivent\",\"donc\",\"dont\",\"douze\",\"douzième\",\"dring\",\"du\",\"duquel\",\"durant\",\"e\",\"effet\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"elles-mêmes\",\"en\",\"encore\",\"entre\",\"envers\",\"environ\",\"es\",\"ès\",\"est\",\"et\",\"etant\",\"étaient\",\"étais\",\"était\",\"étant\",\"etc\",\"été\",\"etre\",\"être\",\"eu\",\"euh\",\"eux\",\"eux-mêmes\",\"excepté\",\"f\",\"façon\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"feront\",\"fi\",\"flac\",\"floc\",\"font\",\"g\",\"gens\",\"h\",\"ha\",\"hé\",\"hein\",\"hélas\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"i\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"k\",\"l\",\"la\",\"là\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lès\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lorsque\",\"lui\",\"lui-même\",\"m\",\"ma\",\"maint\",\"mais\",\"malgré\",\"me\",\"même\",\"mêmes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"moi\",\"moi-même\",\"moins\",\"mon\",\"moyennant\",\"n\",\"na\",\"ne\",\"néanmoins\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"non\",\"nos\",\"notre\",\"nôtre\",\"nôtres\",\"nous\",\"nous-mêmes\",\"nul\",\"o\",\"o|\",\"ô\",\"oh\",\"ohé\",\"olé\",\"ollé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"où\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"p\",\"paf\",\"pan\",\"par\",\"parmi\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"personne\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"plein\",\"plouf\",\"plus\",\"plusieurs\",\"plutôt\",\"pouah\",\"pour\",\"pourquoi\",\"premier\",\"première\",\"premièrement\",\"près\",\"proche\",\"psitt\",\"puisque\",\"q\",\"qu\",\"quand\",\"quant\",\"quanta\",\"quant-à-soi\",\"quarante\",\"quatorze\",\"quatre\",\"quatre-vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelque\",\"quelques\",\"quelqu'un\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"seize\",\"selon\",\"sept\",\"septième\",\"sera\",\"seront\",\"ses\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soit\",\"soixante\",\"son\",\"sont\",\"sous\",\"stop\",\"suis\",\"suivant\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tant\",\"te\",\"té\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutes\",\"treize\",\"trente\",\"très\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"tsoin\",\"tsouin\",\"tu\",\"u\",\"un\",\"une\",\"unes\",\"uns\",\"v\",\"va\",\"vais\",\"vas\",\"vé\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vôtre\",\"vôtres\",\"vous\",\"vous-mêmes\",\"vu\",\"w\",\"x\",\"y\",\"z\",\"zut\",\"alors\",\"aucuns\",\"bon\",\"devrait\",\"dos\",\"droite\",\"début\",\"essai\",\"faites\",\"fois\",\"force\",\"haut\",\"ici\",\"juste\",\"maintenant\",\"mine\",\"mot\",\"nommés\",\"nouveaux\",\"parce\",\"parole\",\"personnes\",\"pièce\",\"plupart\",\"seulement\",\"soyez\",\"sujet\",\"tandis\",\"valeur\",\"voie\",\"voient\",\"état\",\"étions\"]\n",
        "STOPWORDS = list(map(remove_accents,STOPWORDS))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(\"sample_.json\",'r',encoding='cp1252') as json_file:\n",
        "    #json_file.seek(0)\n",
        "    LISTE = json.load(json_file)\n",
        "my_stemmer = lambda word: LISTE[word] if word in LISTE else word\n",
        "\n",
        "def SENTENCE_TO_CORRECT_WORDS(sentence):\n",
        "    \"cette fonction retourne la liste des mots du user\"\n",
        "    print('\\n------------pre_process--------\\n')\n",
        "    liste_words = pre_process(sentence)\n",
        "    print(liste_words)\n",
        "    print('\\n------------correction--------\\n')\n",
        "    liste_words = list(map(CORRECTION,liste_words))\n",
        "    print(liste_words)\n",
        "    print('\\n------------stemming--------\\n')\n",
        "    liste_words = list(map(my_stemmer,liste_words))\n",
        "    print(liste_words)\n",
        "    print('\\n------------remove stop-words--------\\n')\n",
        "    liste_words = [elt for elt in liste_words if elt not in STOPWORDS]\n",
        "    print(liste_words)\n",
        "    print('\\n-------------------------------------\\n')\n",
        "    return liste_words\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_M0vhRqYLAx"
      },
      "source": [
        "# **test**\n",
        "---\n",
        "*importance: canTest*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJFni2QeYJd9",
        "outputId": "37e793bb-8ed4-45b6-acbb-ee4909383940"
      },
      "source": [
        "\n",
        "out = 0 \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('\\n-------------------------------------\\n')\n",
        "    sentence = 'voilà ma phrase'\n",
        "    print('sentence: ',sentence)\n",
        "    liste_words = SENTENCE_TO_CORRECT_WORDS(sentence)\n",
        "    print('liste_words:',liste_words)\n",
        "    print('\\n-------------------------------------\\n')\n",
        "    print('\\ndes phrases à mots raté, d\\'une faute ou deux, à corriger')\n",
        "    while out!=2:\n",
        "        sentence = input('sentence or word: ')\n",
        "        \n",
        "        if sentence: \n",
        "            #CORRECTION(word.lower())\n",
        "            liste_words = SENTENCE_TO_CORRECT_WORDS(sentence)\n",
        "            #print(liste_words)\n",
        "        else: out +=1\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------------------\n",
            "\n",
            "sentence:  voilà ma phrase\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['voila', 'phrase']\n",
            "['voila', 'phrase']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['voila', 'phrase']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['voiler', 'phrase']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['voiler', 'phrase']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "liste_words: ['voiler', 'phrase']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "des phrases à mots raté, d'une faute ou deux, à corriger\n",
            "sentence or word: \n",
            "sentence or word: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhyW0rE3miMo",
        "outputId": "023f29b5-cd5b-4ff5-e229-8b4bbdcfe4d3"
      },
      "source": [
        "SENTENCE_TO_CORRECT_WORDS('messaeges d\\'erreues. Le founiseur ARIBA n\\'existeaint pas')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['messaeges', 'erreues', 'founiseur', 'ariba', 'existeaint', 'pas']\n",
            "['messaeges', 'erreues', 'founiseur', 'ariba', 'existeaint', 'pas']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['messages', 'erreurs', 'founisseur', 'ariba', 'existaient', 'pas']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'founisseur', 'ariba', 'exister', 'pas']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'founisseur', 'ariba', 'exister']\n",
            "\n",
            "-------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['message', 'erreur', 'founisseur', 'ariba', 'exister']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RggFlxIzaga"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj-3IwHIVcXW"
      },
      "source": [
        "# **1-2: un exemple de lemmatizer**\n",
        "---\n",
        "*importance: run if `cosine_similarity_T(mine=False)`*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oj2pUkg30Jai",
        "outputId": "8d259db4-e2e9-4ed3-f0db-92aeb278fe2a"
      },
      "source": [
        "import nltk\n",
        "'''\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nnltk.download('punkt')\\nnltk.download('wordnet')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('stopwords')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ayOh2oCtRGn"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import operator\n",
        "import nltk \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
        "def wordLemmatizer(data,colname):\n",
        "    tag_map = defaultdict(lambda : wn.NOUN)\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB\n",
        "    tag_map['R'] = wn.ADV\n",
        "    file_clean_k =pd.DataFrame()\n",
        "    for index,entry in enumerate(data):\n",
        "        \n",
        "        # Declaring Empty List to store the words that follow the rules for this step\n",
        "        Final_words = []\n",
        "        # Initializing WordNetLemmatizer()\n",
        "        word_Lemmatized = WordNetLemmatizer()\n",
        "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
        "        for word, tag in pos_tag(entry):\n",
        "            # Below condition is to check for Stop words and consider only alphabets\n",
        "            if len(word)>1 and word not in stopwords.words('french') and word.isalpha():\n",
        "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "                Final_words.append(word_Final)\n",
        "            # The final processed set of words for each iteration will be stored in 'text_final'\n",
        "                file_clean_k.loc[index,colname] = str(Final_words)\n",
        "                file_clean_k.loc[index,colname] = str(Final_words)\n",
        "                file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n",
        "                file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n",
        "                file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n",
        "                file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n",
        "\n",
        "    return file_clean_k\n",
        "\n",
        "\n",
        "def wordLemmatizer_(sentence):\n",
        "    #prendre une phrase que retourner un str (les mots sont separes par des ,)\n",
        "    preprocessed_query = preprocessed_query = re.sub(\"\\W+\", \" \", sentence).strip()\n",
        "    tokens = word_tokenize(str(preprocessed_query))\n",
        "    q_df = pd.DataFrame(columns=['q_clean'])\n",
        "    idx = 0\n",
        "    colname = 'keyword_final'\n",
        "    q_df.loc[idx,'q_clean'] =tokens\n",
        "    print('\\n\\n---inputtoken');print(q_df.q_clean)\n",
        "    print('\\n\\n---outputlemma');print(wordLemmatizer(q_df.q_clean,colname).loc[idx,colname])\n",
        "    return wordLemmatizer(q_df.q_clean,colname).loc[idx,colname]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFMWfbMSSUvl"
      },
      "source": [
        "# **2: trouver la meilleure phrase dans une liste de phrase**\n",
        "---\n",
        "*importance: yes run it*\n",
        "---\n",
        "`fonction: cosine_similarity_T`\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT7a9c3pss34"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import operator\n",
        "import numpy as np \n",
        "import time\n",
        "\n",
        "\n",
        "def init(df_news):\n",
        "  print('init!!!')\n",
        "  ## Create Vocabulary\n",
        "  vocabulary = set()\n",
        "  for doc in df_news.Clean_Keyword:\n",
        "      vocabulary.update(doc.split(','))\n",
        "  vocabulary = list(vocabulary)# Intializating the tfIdf model\n",
        "  tfidf = TfidfVectorizer(vocabulary=vocabulary)# Fit the TfIdf model\n",
        "  tfidf.fit(df_news.Clean_Keyword)# Transform the TfIdf model\n",
        "  tfidf_tran=tfidf.transform(df_news.Clean_Keyword)\n",
        "  globals()['vocabulary'],globals()['tfidf'],globals()['tfidf_tran'] = vocabulary,tfidf,tfidf_tran\n",
        "\n",
        "def gen_vector_T(tokens,df_news,vocabulary,tfidf,tfidf_tran):\n",
        "  Q = np.zeros((len(vocabulary)))    \n",
        "  x= tfidf.transform(tokens)\n",
        "  #print(tokens[0].split(','))\n",
        "  #print(keywords)\n",
        "  for token in tokens[0].split(','):\n",
        "      \n",
        "      try:\n",
        "          ind = vocabulary.index(token)\n",
        "          Q[ind]  = x[0, tfidf.vocabulary_[token]]\n",
        "          print(token,':',ind)\n",
        "      except:\n",
        "          print(token,':','not found')\n",
        "          pass\n",
        "  return Q\n",
        "def cosine_sim(a, b):\n",
        "    if not np.linalg.norm(a) and not np.linalg.norm(b): return -3\n",
        "    if not np.linalg.norm(a):return -1\n",
        "    if not np.linalg.norm(b):return -2\n",
        "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "    return cos_sim   \n",
        "def cosine_similarity_T(k, query,df_news,vocabulary=None,tfidf=None,tfidf_tran=None,mine=True):\n",
        "    try:\n",
        "      vocabulary = globals()['vocabulary']\n",
        "      tfidf = globals()['tfidf']\n",
        "      tfidf_tran = globals()['tfidf_tran']\n",
        "    except:\n",
        "      print('up exception')\n",
        "      init(df_news)\n",
        "    q_df = pd.DataFrame(columns=['q_clean'])\n",
        "    if mine:q_df.loc[0,'q_clean'] =','.join(SENTENCE_TO_CORRECT_WORDS(query))\n",
        "    else:q_df.loc[0,'q_clean'] = wordLemmatizer_(query)\n",
        "    \n",
        "    \n",
        "    print('\\n\\n---q_df');print(q_df)\n",
        "    \n",
        "    print('\\n\\n')\n",
        "    d_cosines = []\n",
        "    query_vector = gen_vector_T(q_df['q_clean'],df_news,vocabulary,tfidf,tfidf_tran )\n",
        "    for d in tfidf_tran.A:\n",
        "        d_cosines.append(cosine_sim(query_vector, d ))\n",
        "                    \n",
        "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
        "    #print(\"\")\n",
        "    d_cosines.sort()\n",
        "    a = pd.DataFrame()\n",
        "    for i,index in enumerate(out):\n",
        "        a.loc[i,'index'] = str(index)\n",
        "        a.loc[i,'Subject'] = df_news['Subject'][index]\n",
        "    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n",
        "        a.loc[j,'Score'] = simScore\n",
        "    return a\n",
        "\n",
        "\n",
        "def test(data,sentence,init_=False,mine=True):\n",
        "  if not init_:\n",
        "    deb = time.time();print('\\n\\n###########')\n",
        "    init(df_news)\n",
        "    print('\\n###########temps init: ', time.time()-deb)\n",
        "  deb = time.time();print('\\n\\n###########')\n",
        "  print(cosine_similarity_T(10, sentence,df_news))\n",
        "  print('\\n###########temps methode 1: ', time.time()-deb)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyOAa9p_R6xh"
      },
      "source": [
        "# **Example1:**\n",
        "---\n",
        "`BDD: df_news`\n",
        "---\n",
        "*importance: nope*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uncpXTX0oWq7"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_bD21rYsTf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b52e12-8c93-44f4-acab-513083f48aa6"
      },
      "source": [
        "import pandas as pd \n",
        "import re \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "news = pd.read_json('https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json')\n",
        "\n",
        "\n",
        "\n",
        "for i,txt in enumerate(news['content']):\n",
        "    subject = re.findall('Subject:(.*\\n)',txt)\n",
        "    if (len(subject) !=0):\n",
        "        news.loc[i,'Subject'] =str(i)+' '+subject[0]\n",
        "    else:\n",
        "        news.loc[i,'Subject'] ='NA'\n",
        "df_news =news[['Subject','content']]\n",
        "\n",
        "\n",
        "\n",
        "df_news.content =df_news.content.replace(to_replace='from:(.*\\n)',value='',regex=True) ##remove from to email \n",
        "df_news.content =df_news.content.replace(to_replace='lines:(.*\\n)',value='',regex=True)\n",
        "df_news.content =df_news.content.replace(to_replace='[!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except\n",
        "df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)\n",
        "df_news.content =df_news.content.replace(to_replace='\\s+',value=' ',regex=True)    #remove new line\n",
        "df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space\n",
        "df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace\n",
        "\n",
        "\n",
        "\n",
        "df_news['content']=[entry.lower() for entry in df_news['content']]\n",
        "df_news['Word tokenize']= [word_tokenize(entry) for entry in df_news.content]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63c5WgzgtRB5"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIZIjS6ky8X1"
      },
      "source": [
        "# **Example1: plus rapide**\n",
        "---\n",
        "`BDD: df_news`\n",
        "---\n",
        "*importance: canTest*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "rQEkNTG2y_wC",
        "outputId": "762c5a2a-23dd-4569-f271-f678c38b9dd8"
      },
      "source": [
        "import pandas as pd \n",
        "df_news = pd.read_json('https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json')\n",
        "df_news.head()\n",
        "df_news.info()\n",
        "df_news.loc[0,'Clean_Keyword']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 11314 entries, 0 to 11313\n",
            "Data columns (total 2 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Subject        11314 non-null  object\n",
            " 1   Clean_Keyword  11314 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 265.2+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'thing,car,nntp,post,host,university,maryland,college,park,line,wonder,anyone,could,enlighten,car,saw,day,sport,car,look,late,early,call,bricklin,door,really,small,addition,front,bumper,separate,rest,body,know,anyone,tellme,model,name,engine,spec,year,production,car,make,history,whatever,info,funky,look,car,please,mail,il,bring,neighborhood,lerxst'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_0jJ7wdSHgb"
      },
      "source": [
        "# **Text example1**\n",
        "---\n",
        "`output: mesures de similarités`\n",
        "---\n",
        "*importance: canTest*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in_yInEsA0gN",
        "outputId": "379b8bb5-bef9-44c8-a23d-aadd7ef32d3e"
      },
      "source": [
        "init"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.init>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB55Q3gQp2JP",
        "outputId": "cc245f2d-eec8-4af1-e99d-54c728dd241f"
      },
      "source": [
        "sentence = \"computer science\"\n",
        "test(df_news,sentence,init_=False,mine=True) #il v bugger sur l'anglais à moins que je remplace les data (stop_words et corpus et tout)\n",
        "sentence = \"state diagrams\"\n",
        "test(df_news,sentence,init_=True,mine=True) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "###########\n",
            "init!!!\n",
            "\n",
            "###########temps init:  3.1772680282592773\n",
            "\n",
            "\n",
            "###########\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['computer', 'science']\n",
            "['computer', 'science']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['computer', 'science']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['computer', 'science']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['computer', 'science']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "---q_df\n",
            "            q_clean\n",
            "0  computer,science\n",
            "\n",
            "\n",
            "\n",
            "computer : 39988\n",
            "science : 7641\n",
            "   index                                            Subject     Score\n",
            "0   2231             2231 Computer Engr vs Computer Science  0.396137\n",
            "1  10340                      10340 Science and Methodology  0.302603\n",
            "2   4173                   4173 Rawlins debunks creationism  0.284936\n",
            "3   4326             4326 Computer Engr vs Computer Science  0.284256\n",
            "4   6921            6921 Automatic layout of state diagrams  0.260732\n",
            "5   7618  7618 Solution Why do I need xrdb m when Xdefau...  0.251170\n",
            "6   5741                   5741 Rawlins debunks creationism  0.247442\n",
            "7   8464                                 8464 Date is stuck  0.238985\n",
            "8   3177    3177 Homeopathy a respectable medical tradition  0.235738\n",
            "9   4366             4366 Computer Engr vs Computer Science  0.227186\n",
            "\n",
            "###########temps methode 1:  10.28007435798645\n",
            "\n",
            "\n",
            "###########\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['state', 'diagrams']\n",
            "['state', 'diagrams']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['tate', 'diagramme']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['tater', 'diagramme']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['tater', 'diagramme']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "---q_df\n",
            "           q_clean\n",
            "0  tater,diagramme\n",
            "\n",
            "\n",
            "\n",
            "tater : 35726\n",
            "diagramme : not found\n",
            "  index                                            Subject     Score\n",
            "0  6375                           6375 Early BBDDD Returns  0.170922\n",
            "1  3711                           3711 Early BBDDD Returns  0.165878\n",
            "2  3982   3982 Binaca Blast Deep Drive Derby BBDDD Returns  0.075045\n",
            "3  3775                         3775 Analog switches Balan  0.000000\n",
            "4  3774          3774 A question that has bee bothering me  0.000000\n",
            "5  3773  3773 After 2000 years can we say that Christia...  0.000000\n",
            "6  3772                   3772 CR ROM Drive Recommendation  0.000000\n",
            "7  3771                                  3771 Windows Help  0.000000\n",
            "8  3770       3770 AF ATS Red Army Fraction RAF communique  0.000000\n",
            "9  3769            3769 With a surge in the last two weeks  0.000000\n",
            "\n",
            "###########temps methode 1:  3.9334299564361572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58v4G7RBB1x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvXkfg1QBuI-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR8LwLhYSoQj"
      },
      "source": [
        "# **Example2:**\n",
        "---\n",
        "`BDD: df_new`\n",
        "---\n",
        "*importance: yes: run it*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7zq_zxQTl0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c83eeba-9e6f-443f-af6a-fb86ef2f2537"
      },
      "source": [
        "def open_file(textdir):\n",
        "  found = False\n",
        "  try:texte = open(textdir,'r',encoding=\"utf-8\").read();found=True\n",
        "  except:pass\n",
        "  try: texte = open(textdir,'r').read();found=True \n",
        "  except: pass\n",
        "  if not found:\n",
        "    texte = open(textdir,'r',encoding='cp1252').read();found=True\n",
        "  return  texte\n",
        "def add_col(df_news,titre,keywords):\n",
        "  return df_news.append(dict(zip(df_news.columns,[titre, keywords])), ignore_index=True)\n",
        "SENTENCE_TO_CORRECT_WORDS('La PR reste au statut «\\xa0Approuve(e)\\xa0» et il n’y a pas de commande\\\"\\'')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['reste', 'statut', 'approuve', 'n’y', 'pas', 'commande']\n",
            "['reste', 'statut', 'approuve', 'n’y', 'pas', 'commande']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['reste', 'statut', 'approuve', 'n’y', 'pas', 'commande']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['rester', 'statut', 'approuver', 'n’y', 'pas', 'commander']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['rester', 'statut', 'approuver', 'n’y', 'commander']\n",
            "\n",
            "-------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rester', 'statut', 'approuver', 'n’y', 'commander']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEdPkpsks1DW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d2a88e1-a157-4d89-9410-18acccb08bd3"
      },
      "source": [
        "liste_pb = [elt for elt in open_file('liste_pb_.txt').split('\\n') if elt]\n",
        "df_new = df_news.drop(df_news.index)\n",
        "for i,titre in enumerate(liste_pb):\n",
        "  keywords = ','.join(SENTENCE_TO_CORRECT_WORDS(titre))\n",
        "  df_new = add_col(df_new,titre,keywords)\n",
        "df_new.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'fournisseur', 'ariba', 'existe', 'pas']\n",
            "['message', 'erreur', 'fournisseur', 'ariba', 'existe', 'pas']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'ariba', 'existe', 'pas']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'ariba', 'exister', 'pas']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'ariba', 'exister']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'commande', 'd’article', 'non', 'autorise', 'otp']\n",
            "['message', 'erreur', 'commande', 'd’article', 'non', 'autorise', 'otp']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'commande', 'd’article', 'non', 'autorise', 'otp']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'commander', 'd’article', 'non', 'autoriser', 'otp']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'commander', 'd’article', 'autoriser', 'otp']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'statut', 'systeme', 'tclo', 'actif', 'ord']\n",
            "['message', 'erreur', 'statut', 'systeme', 'tclo', 'actif', 'ord']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'systeme', 'tclo', 'actif', 'ord']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'systeme', 'tclo', 'actif', 'ord']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'systeme', 'tclo', 'actif', 'ord']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'cost', 'center', 'change', 'could', 'not', 'effected']\n",
            "['message', 'erreur', 'cost', 'center', 'change', 'could', 'not', 'effected']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'cost', 'center', 'change', 'could', 'not', 'effected']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'cost', 'center', 'changer', 'could', 'not', 'effected']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'cost', 'center', 'changer', 'could', 'not', 'effected']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['messaeg', 'erreur', 'otp', 'change', 'could', 'not', 'effected']\n",
            "['messaeg', 'erreur', 'otp', 'change', 'could', 'not', 'effected']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['messaeg', 'erreur', 'otp', 'change', 'could', 'not', 'effected']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['messaeg', 'erreur', 'otp', 'changer', 'could', 'not', 'effected']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['messaeg', 'erreur', 'otp', 'changer', 'could', 'not', 'effected']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['messaeg', 'erreur', 'entrez', 'centre', 'couts']\n",
            "['messaeg', 'erreur', 'entrez', 'centre', 'couts']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['messaeg', 'erreur', 'entrez', 'centre', 'couts']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['messaeg', 'erreur', 'entrer', 'centrer', 'cout']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['messaeg', 'erreur', 'entrer', 'centrer', 'cout']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'indiquez', 'une', 'seule', 'imputation', 'non', 'statistique']\n",
            "['message', 'erreur', 'indiquez', 'une', 'seule', 'imputation', 'non', 'statistique']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'indiquez', 'une', 'seule', 'imputation', 'non', 'statistique']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'indiquer', 'un', 'seul', 'imputation', 'non', 'statistique']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'indiquer', 'seul', 'imputation', 'statistique']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'imputations', 'ont', 'des', 'centres', 'profit', 'differents']\n",
            "['message', 'erreur', 'imputations', 'ont', 'des', 'centres', 'profit', 'differents']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'imputations', 'ont', 'des', 'centres', 'profit', 'differents']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'imputation', 'ont', 'de', 'centrer', 'profit', 'different']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'imputation', 'centrer', 'profit']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'poste', 'ordre', 'depassement', 'budget']\n",
            "['message', 'erreur', 'poste', 'ordre', 'depassement', 'budget']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'poste', 'ordre', 'depassement', 'budget']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'poster', 'ordre', 'depassement', 'budget']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'poster', 'ordre', 'depassement', 'budget']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'entrez', 'une', 'quantite', 'commande']\n",
            "['message', 'erreur', 'entrez', 'une', 'quantite', 'commande']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'entrez', 'une', 'quantite', 'commande']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'entrer', 'un', 'quantite', 'commander']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'entrer', 'quantite', 'commander']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'indiquez', 'quantite']\n",
            "['message', 'erreur', 'indiquez', 'quantite']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'indiquez', 'quantite']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'indiquer', 'quantite']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'indiquer', 'quantite']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'prix', 'net', 'doit', 'etre', 'superieur']\n",
            "['message', 'erreur', 'prix', 'net', 'doit', 'etre', 'superieur']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'prix', 'net', 'doit', 'etre', 'superieur']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'prix', 'net', 'doit', 'etre', 'superieur']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'prix', 'net', 'superieur']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'les', 'conditions', 'paiement', 'sont', 'pas', 'creees']\n",
            "['message', 'erreur', 'les', 'conditions', 'paiement', 'sont', 'pas', 'creees']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'les', 'conditions', 'paiement', 'sont', 'pas', 'creees']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'le', 'condition', 'paiement', 'sont', 'pas', 'creer']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'condition', 'paiement', 'creer']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'centre', 'profit', 'existe', 'pas', 'perimetre', 'comptabilis']\n",
            "['message', 'erreur', 'centre', 'profit', 'existe', 'pas', 'perimetre', 'comptabilis']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'centre', 'profit', 'existe', 'pas', 'perimetre', 'comptabilis']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'centrer', 'profit', 'exister', 'pas', 'perimetre', 'comptabilis']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'centrer', 'profit', 'exister', 'perimetre', 'comptabilis']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  []\n",
            "[]\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "[]\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "[]\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "[]\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'utilisateur', 'traite', 'deja', 'cde', 'achat']\n",
            "['message', 'erreur', 'utilisateur', 'traite', 'deja', 'cde', 'achat']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'utilisateur', 'traite', 'deja', 'cde', 'achat']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'utilisateur', 'traiter', 'deja', 'cde', 'achat']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'utilisateur', 'traiter', 'deja', 'cde', 'achat']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'validation', 'compte', 'est', 'interdit', 'sur', 'centre']\n",
            "['message', 'erreur', 'validation', 'compte', 'est', 'interdit', 'sur', 'centre']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'validation', 'compte', 'est', 'interdit', 'sur', 'centre']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'validation', 'compter', 'est', 'interdire', 'sur', 'centrer']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'validation', 'compter', 'interdire', 'centrer']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'validation', 'compte', 'doit', 'etre', 'impute', 'sur', 'centre', 'cout']\n",
            "['message', 'erreur', 'validation', 'compte', 'doit', 'etre', 'impute', 'sur', 'centre', 'cout']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'validation', 'compte', 'doit', 'etre', 'impute', 'sur', 'centre', 'cout']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'validation', 'compter', 'doit', 'etre', 'imputer', 'sur', 'centrer', 'cout']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'validation', 'compter', 'imputer', 'centrer', 'cout']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'fournisseur', 'mdm', 'n’existe', 'pas']\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'n’existe', 'pas']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'n’existe', 'pas']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'n’existe', 'pas']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'n’existe']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'fournisseur', 'mdm', 'est', 'bloque', 'pour', 'l’organisation', 'achats']\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'est', 'bloque', 'pour', 'l’organisation', 'achats']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'est', 'bloque', 'pour', 'l’organisation', 'achats']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'est', 'bloquer', 'pour', 'l’organisation', 'achat']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'mdm', 'bloquer', 'l’organisation', 'achat']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'date', 'livraison', 'situee', 'dans', 'passe']\n",
            "['message', 'erreur', 'date', 'livraison', 'situee', 'dans', 'passe']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'date', 'livraison', 'situee', 'dans', 'passe']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'dater', 'livraison', 'situer', 'dan', 'passer']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'dater', 'livraison', 'situer', 'dan', 'passer']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'qte', 'livree', 'est', 'differente', 'qte', 'facturee', 'fonction', 'impossible']\n",
            "['message', 'erreur', 'qte', 'livree', 'est', 'differente', 'qte', 'facturee', 'fonction', 'impossible']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'livree', 'est', 'differente', 'qte', 'facturee', 'fonction', 'impossible']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'livrer', 'est', 'different', 'qte', 'facturer', 'fonction', 'impossible']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'livrer', 'qte', 'facturer', 'fonction', 'impossible']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'groupe', 'd’acheteurs', 'not', 'non', 'defini', 'verifiez', 'vos', 'donnees']\n",
            "['message', 'erreur', 'groupe', 'd’acheteurs', 'not', 'non', 'defini', 'verifiez', 'vos', 'donnees']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'groupe', 'd’acheteurs', 'not', 'non', 'defini', 'verifiez', 'vos', 'donnees']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'grouper', 'd’acheteurs', 'not', 'non', 'definir', 'verifier', 'votre', 'donnee']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'grouper', 'd’acheteurs', 'not', 'definir', 'verifier', 'donnee']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'founisseur', 'xxxx', 'n’existe', 'pas']\n",
            "['message', 'erreur', 'founisseur', 'xxxx', 'n’existe', 'pas']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'founisseur', 'xxxx', 'n’existe', 'pas']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'founisseur', 'xxxx', 'n’existe', 'pas']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'founisseur', 'xxxx', 'n’existe']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'cpte', 'general', 'peut', 'etre', 'utilise', 'corrigez']\n",
            "['message', 'erreur', 'cpte', 'general', 'peut', 'etre', 'utilise', 'corrigez']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'cpte', 'general', 'peut', 'etre', 'utilise', 'corrigez']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'cpte', 'general', 'peut', 'etre', 'utiliser', 'corriger']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'cpte', 'general', 'utiliser', 'corriger']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'erreur', 'conversion', 'des', 'quantites', 'lors', 'calcul', 'prix', 'net']\n",
            "['message', 'erreur', 'erreur', 'conversion', 'des', 'quantites', 'lors', 'calcul', 'prix', 'net']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'erreur', 'conversion', 'des', 'quantites', 'lors', 'calcul', 'prix', 'net']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'erreur', 'conversion', 'de', 'quantite', 'lors', 'calcul', 'prix', 'net']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'erreur', 'conversion', 'quantite', 'lors', 'calcul', 'prix', 'net']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'qte', 'tolerance', 'livr', 'excedentaire', 'inferieur', 'quantite', 'livree']\n",
            "['message', 'erreur', 'qte', 'tolerance', 'livr', 'excedentaire', 'inferieur', 'quantite', 'livree']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'tolerance', 'livr', 'excedentaire', 'inferieur', 'quantite', 'livree']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'tolerance', 'livr', 'excedentaire', 'inferieur', 'quantite', 'livrer']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'tolerance', 'livr', 'excedentaire', 'inferieur', 'quantite', 'livrer']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'fonctions', 'oblig', 'suivantes', 'non', 'def', 'gestion', 'des', 'partenaires']\n",
            "['message', 'erreur', 'fonctions', 'oblig', 'suivantes', 'non', 'def', 'gestion', 'des', 'partenaires']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'fonctions', 'oblig', 'suivantes', 'non', 'def', 'gestion', 'des', 'partenaires']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'fonction', 'oblig', 'suivant', 'non', 'def', 'gestion', 'de', 'partenaire']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'fonction', 'oblig', 'def', 'gestion', 'partenaire']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'article', 'non', 'gere', 'dans', 'division']\n",
            "['message', 'erreur', 'article', 'non', 'gere', 'dans', 'division']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'article', 'non', 'gere', 'dans', 'division']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'article', 'non', 'gerer', 'dan', 'division']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'article', 'gerer', 'dan', 'division']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'renseigner', 'correctement', 'demandeur']\n",
            "['message', 'erreur', 'renseigner', 'correctement', 'demandeur']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'renseigner', 'correctement', 'demandeur']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'renseigner', 'correctement', 'demandeur']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'renseigner', 'correctement', 'demandeur']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'l’unite', 'quantite', 'par', 'n’est', 'pas', 'definie', 'dans', 'langue']\n",
            "['message', 'erreur', 'l’unite', 'quantite', 'par', 'n’est', 'pas', 'definie', 'dans', 'langue']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'l’unite', 'quantite', 'par', 'n’est', 'pas', 'definie', 'dans', 'langue']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'l’unite', 'quantite', 'par', 'n’est', 'pas', 'definir', 'dan', 'langue']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'l’unite', 'quantite', 'n’est', 'definir', 'dan', 'langue']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['quantite', 'est', 'superieure', 'quantite', 'disponible']\n",
            "['quantite', 'est', 'superieure', 'quantite', 'disponible']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['quantite', 'est', 'superieure', 'quantite', 'disponible']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['quantite', 'est', 'superieur', 'quantite', 'disponible']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['quantite', 'superieur', 'quantite', 'disponible']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'existe', 'des', 'factures', 'pour', 'cas', 'creation', 'mouvement', 'impossible']\n",
            "['message', 'erreur', 'existe', 'des', 'factures', 'pour', 'cas', 'creation', 'mouvement', 'impossible']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'existe', 'des', 'factures', 'pour', 'cas', 'creation', 'mouvement', 'impossible']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'exister', 'de', 'facturer', 'pour', 'cas', 'creation', 'mouvement', 'impossible']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'exister', 'facturer', 'cas', 'creation', 'mouvement', 'impossible']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'qte', 'entre', 'facture', 'pas', 'atteint', 'difference']\n",
            "['message', 'erreur', 'qte', 'entre', 'facture', 'pas', 'atteint', 'difference']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'entre', 'facture', 'pas', 'atteint', 'difference']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'entrer', 'facturer', 'pas', 'atteindre', 'difference']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'entrer', 'facturer', 'atteindre', 'difference']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'qte', 'entree', 'march', 'pas', 'atteint', 'difference']\n",
            "['message', 'erreur', 'qte', 'entree', 'march', 'pas', 'atteint', 'difference']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'entree', 'march', 'pas', 'atteint', 'difference']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'entrer', 'march', 'pas', 'atteindre', 'difference']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'entrer', 'march', 'atteindre', 'difference']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'qte', 'commandee', 'depasse']\n",
            "['message', 'erreur', 'qte', 'commandee', 'depasse']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'commandee', 'depasse']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'commander', 'depasser']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'qte', 'commander', 'depasser']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'ecriture', 'impossible', 'immob', 'mise', 'hors', 'service']\n",
            "['message', 'erreur', 'ecriture', 'impossible', 'immob', 'mise', 'hors', 'service']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'ecriture', 'impossible', 'immob', 'mise', 'hors', 'service']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'ecriture', 'impossible', 'immob', 'miser', 'hors', 'service']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'ecriture', 'impossible', 'immob', 'miser', 'service']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'quantite', 'totale', 'est', 'differente', 'celle', 'controlee', 'douan']\n",
            "['message', 'erreur', 'quantite', 'totale', 'est', 'differente', 'celle', 'controlee', 'douan']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'quantite', 'totale', 'est', 'differente', 'celle', 'controlee', 'douan']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'quantite', 'total', 'est', 'different', 'celui', 'controler', 'douan']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'quantite', 'total', 'controler', 'douan']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'date', 'comptable', 'interdite', 'pour', 'saisie', 'receptions']\n",
            "['message', 'erreur', 'date', 'comptable', 'interdite', 'pour', 'saisie', 'receptions']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'date', 'comptable', 'interdite', 'pour', 'saisie', 'receptions']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'dater', 'comptable', 'interdire', 'pour', 'saisir', 'reception']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'dater', 'comptable', 'interdire', 'saisir', 'reception']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'enregistrer', 'uniq', 'les', 'periodes', 'possible', 'dom', 'val']\n",
            "['message', 'erreur', 'enregistrer', 'uniq', 'les', 'periodes', 'possible', 'dom', 'val']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'enregistrer', 'uniq', 'les', 'periodes', 'possible', 'dom', 'val']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'enregistrer', 'uniq', 'le', 'periode', 'possible', 'dom', 'val']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'enregistrer', 'uniq', 'periode', 'possible', 'dom', 'val']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'periode', 'n’est', 'pas', 'ouverte', 'pour', 'type', 'dpte']\n",
            "['message', 'erreur', 'periode', 'n’est', 'pas', 'ouverte', 'pour', 'type', 'dpte']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'periode', 'n’est', 'pas', 'ouverte', 'pour', 'type', 'dpte']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'periode', 'n’est', 'pas', 'ouvrir', 'pour', 'typer', 'dpte']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'periode', 'n’est', 'ouvrir', 'typer', 'dpte']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'document', 'contient', 'aucun', 'poste']\n",
            "['message', 'erreur', 'document', 'contient', 'aucun', 'poste']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'document', 'contient', 'aucun', 'poste']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'document', 'contenir', 'aucun', 'poster']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'document', 'contenir', 'poster']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'aucune', 'entree', 'stock', 'possible', 'pour', 'commande']\n",
            "['message', 'erreur', 'aucune', 'entree', 'stock', 'possible', 'pour', 'commande']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'aucune', 'entree', 'stock', 'possible', 'pour', 'commande']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'aucun', 'entrer', 'stock', 'possible', 'pour', 'commander']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'entrer', 'stock', 'possible', 'commander']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'entree', 'marchandises', 'commande', 'non', 'autorise', 'otp']\n",
            "['message', 'erreur', 'entree', 'marchandises', 'commande', 'non', 'autorise', 'otp']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'entree', 'marchandises', 'commande', 'non', 'autorise', 'otp']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'entrer', 'marchandise', 'commander', 'non', 'autoriser', 'otp']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'entrer', 'marchandise', 'commander', 'autoriser', 'otp']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'centre', 'est', 'bloque', 'aux', 'ecritures', 'couts', 'primaires']\n",
            "['message', 'erreur', 'centre', 'est', 'bloque', 'aux', 'ecritures', 'couts', 'primaires']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'centre', 'est', 'bloque', 'aux', 'ecritures', 'couts', 'primaires']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'centrer', 'est', 'bloquer', 'aux', 'ecriture', 'cout', 'primaire']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'centrer', 'bloquer', 'ecriture', 'cout', 'primaire']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'statut', 'utilisateur', 'zbha', 'actif', 'otp']\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'zbha', 'actif', 'otp']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'zbha', 'actif', 'otp']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'zbha', 'actif', 'otp']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'zbha', 'actif', 'otp']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'statut', 'systeme', 'bloq', 'actif', 'otp']\n",
            "['message', 'erreur', 'statut', 'systeme', 'bloq', 'actif', 'otp']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'systeme', 'bloq', 'actif', 'otp']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'systeme', 'bloq', 'actif', 'otp']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'systeme', 'bloq', 'actif', 'otp']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'statut', 'utilisateur', 'ferm', 'actif', 'otp']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'ecritures', 'd’immobilisations', 'pour', 'ste']\n",
            "['message', 'erreur', 'ecritures', 'd’immobilisations', 'pour', 'ste']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'ecritures', 'd’immobilisations', 'pour', 'ste']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'ecriture', 'd’immobilisations', 'pour', 'ste']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'ecriture', 'd’immobilisations', 'ste']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'les', 'donnees', 'division', 'article', 'sont', 'bloquees', 'par', 'utilisateur']\n",
            "['message', 'erreur', 'les', 'donnees', 'division', 'article', 'sont', 'bloquees', 'par', 'utilisateur']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'les', 'donnees', 'division', 'article', 'sont', 'bloquees', 'par', 'utilisateur']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'le', 'donnee', 'division', 'article', 'sont', 'bloquer', 'par', 'utilisateur']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'donnee', 'division', 'article', 'bloquer', 'utilisateur']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'utilisateur', 'yraribatch', 'traite', 'deja', 'poste', 'document', 'achat']\n",
            "['message', 'erreur', 'utilisateur', 'yraribatch', 'traite', 'deja', 'poste', 'document', 'achat']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'utilisateur', 'yraribatch', 'traite', 'deja', 'poste', 'document', 'achat']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'utilisateur', 'yraribatch', 'traiter', 'deja', 'poster', 'document', 'achat']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'utilisateur', 'yraribatch', 'traiter', 'deja', 'poster', 'document', 'achat']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'tableau', 'valeur', 'acquisition', 'devient', 'negative']\n",
            "['message', 'erreur', 'tableau', 'valeur', 'acquisition', 'devient', 'negative']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'tableau', 'valeur', 'acquisition', 'devient', 'negative']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'tableau', 'valeur', 'acquisition', 'devier', 'negatif']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'tableau', 'acquisition', 'devier', 'negatif']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'fournisseur', 'non', 'present', 'dans', 'ariba', 'mais', 'present', 'dans', 'sap']\n",
            "['message', 'erreur', 'fournisseur', 'non', 'present', 'dans', 'ariba', 'mais', 'present', 'dans', 'sap']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'non', 'present', 'dans', 'ariba', 'mais', 'present', 'dans', 'sap']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'non', 'present', 'dan', 'ariba', 'mais', 'present', 'dan', 'sap']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'fournisseur', 'present', 'dan', 'ariba', 'present', 'dan', 'sap']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'vendeur', 'ete', 'supprime', 'modification']\n",
            "['message', 'erreur', 'vendeur', 'ete', 'supprime', 'modification']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'vendeur', 'ete', 'supprime', 'modification']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'vendeur', 'ete', 'supprimer', 'modification']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'vendeur', 'supprimer', 'modification']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['message', 'erreur', 'reste', 'statut', 'approuve', 'n’y', 'pas', 'commande']\n",
            "['message', 'erreur', 'reste', 'statut', 'approuve', 'n’y', 'pas', 'commande']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['message', 'erreur', 'reste', 'statut', 'approuve', 'n’y', 'pas', 'commande']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['message', 'erreur', 'rester', 'statut', 'approuver', 'n’y', 'pas', 'commander']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['message', 'erreur', 'rester', 'statut', 'approuver', 'n’y', 'commander']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['ppp']\n",
            "['ppp']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['ppp']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['ppp']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['ppp']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo4', 'commande', 'envoi', 'une', 'commande', 'manuelle']\n",
            "['uo4', 'commande', 'envoi', 'une', 'commande', 'manuelle']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo4', 'commande', 'envoi', 'une', 'commande', 'manuelle']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo4', 'commander', 'envoi', 'un', 'commander', 'manuel']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo4', 'commander', 'envoi', 'commander', 'manuel']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo5', 'reception', 'anomalie', 'workflow']\n",
            "['uo5', 'reception', 'anomalie', 'workflow']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo5', 'reception', 'anomalie', 'workflow']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo5', 'reception', 'anomalie', 'workflow']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo5', 'reception', 'anomalie', 'workflow']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo5', 'reception', 'modification', 'reception']\n",
            "['uo5', 'reception', 'modification', 'reception']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo5', 'reception', 'modification', 'reception']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo5', 'reception', 'modification', 'reception']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo5', 'reception', 'modification', 'reception']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo5', 'reception', 'annulation', 'reception']\n",
            "['uo5', 'reception', 'annulation', 'reception']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo5', 'reception', 'annulation', 'reception']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo5', 'reception', 'annulation', 'reception']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo5', 'reception', 'annulation', 'reception']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo5', 'reception', 'forcer', 'reception']\n",
            "['uo5', 'reception', 'forcer', 'reception']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo5', 'reception', 'forcer', 'reception']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo5', 'reception', 'forcer', 'reception']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo5', 'reception', 'forcer', 'reception']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo3', 'demande', 'achat', 'demande', 'support', 'creation']\n",
            "['uo3', 'demande', 'achat', 'demande', 'support', 'creation']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo3', 'demande', 'achat', 'demande', 'support', 'creation']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo3', 'demander', 'achat', 'demander', 'support', 'creation']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo3', 'demander', 'achat', 'demander', 'support', 'creation']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo3', 'demande', 'achat', 'demande', 'support', 'modification']\n",
            "['uo3', 'demande', 'achat', 'demande', 'support', 'modification']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo3', 'demande', 'achat', 'demande', 'support', 'modification']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo3', 'demander', 'achat', 'demander', 'support', 'modification']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo3', 'demander', 'achat', 'demander', 'support', 'modification']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo3', 'demande', 'achat', 'demande', 'support', 'annulation']\n",
            "['uo3', 'demande', 'achat', 'demande', 'support', 'annulation']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo3', 'demande', 'achat', 'demande', 'support', 'annulation']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo3', 'demander', 'achat', 'demander', 'support', 'annulation']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo3', 'demander', 'achat', 'demander', 'support', 'annulation']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo4', 'commande', 'demande', 'support', 'modification', 'commande']\n",
            "['uo4', 'commande', 'demande', 'support', 'modification', 'commande']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo4', 'commande', 'demande', 'support', 'modification', 'commande']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo4', 'commander', 'demander', 'support', 'modification', 'commander']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo4', 'commander', 'demander', 'support', 'modification', 'commander']\n",
            "\n",
            "-------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Subject</th>\n",
              "      <th>Clean_Keyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Message d'erreur : \"Le fournisseur ARIBA n'exi...</td>\n",
              "      <td>message,erreur,fournisseur,ariba,exister</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Message d'erreur : \"Commande d’article non aut...</td>\n",
              "      <td>message,erreur,commander,d’article,autoriser,otp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Message d'erreur : \"Statut utilisateur FERM ac...</td>\n",
              "      <td>message,erreur,statut,utilisateur,ferm,actif,otp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Message d'erreur : \"Statut systeme TCLO actif ...</td>\n",
              "      <td>message,erreur,statut,systeme,tclo,actif,ord</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Message d'erreur \"___ Cost center change could...</td>\n",
              "      <td>message,erreur,cost,center,changer,could,not,e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Subject                                      Clean_Keyword\n",
              "0  Message d'erreur : \"Le fournisseur ARIBA n'exi...           message,erreur,fournisseur,ariba,exister\n",
              "1  Message d'erreur : \"Commande d’article non aut...   message,erreur,commander,d’article,autoriser,otp\n",
              "2  Message d'erreur : \"Statut utilisateur FERM ac...   message,erreur,statut,utilisateur,ferm,actif,otp\n",
              "3  Message d'erreur : \"Statut systeme TCLO actif ...       message,erreur,statut,systeme,tclo,actif,ord\n",
              "4  Message d'erreur \"___ Cost center change could...  message,erreur,cost,center,changer,could,not,e..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3whVJKgSzlI"
      },
      "source": [
        "# **Test Example2**\n",
        "---\n",
        "`BDD: df_new`\n",
        "---\n",
        "*importance: yes run it*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "-L1ySClCJeUQ",
        "outputId": "234a5972-60d1-43a7-ce67-d2f1f2dc115f"
      },
      "source": [
        "sentence = 'Message d\\'erreur \\\"La qte livree est differente de la qte facturee ; fonction impossible\"'\n",
        "sentence = 'erreur de conversion'\n",
        "sentence = 'message d\\'erreur'\n",
        "sentence = \"le fournisseur MDM n'existe pas\"\n",
        "sentence = \"groupe d'acheteurs non défini\"\n",
        "sentence = \"UO4\"\n",
        "init(df_new) \n",
        "\n",
        "cosine_similarity_T(10,sentence,df_new )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init!!!\n",
            "\n",
            "------------pre_process--------\n",
            "\n",
            "\n",
            "sentence to words :  ['uo4']\n",
            "['uo4']\n",
            "\n",
            "------------correction--------\n",
            "\n",
            "['uo4']\n",
            "\n",
            "------------stemming--------\n",
            "\n",
            "['uo4']\n",
            "\n",
            "------------remove stop-words--------\n",
            "\n",
            "['uo4']\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "---q_df\n",
            "  q_clean\n",
            "0     uo4\n",
            "\n",
            "\n",
            "\n",
            "uo4 : 155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65</td>\n",
              "      <td>UO4-2 Commande | Demande de support modificati...</td>\n",
              "      <td>0.428515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57</td>\n",
              "      <td>UO4-5 Commande | Envoi d'une commande manuelle</td>\n",
              "      <td>0.424080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30</td>\n",
              "      <td>Message d'erreur \"Renseigner correctement le d...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>29</td>\n",
              "      <td>Message d'erreur \"Article ___ non gere dans la...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Message d'erreur \"Fonctions oblig. Suivantes n...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>27</td>\n",
              "      <td>Message d'erreur \"Qte (+ tolerance livr.excede...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>26</td>\n",
              "      <td>Message d'erreur \"Erreur de conversion des qua...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>25</td>\n",
              "      <td>Message d'erreur \"Le cpte general ___ ne peut ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>16</td>\n",
              "      <td>Message d'erreur \"Utilisateur ___ traite deja ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>24</td>\n",
              "      <td>Message d'erreur \"Le founisseur XXXX-40 n’exis...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  index                                            Subject     Score\n",
              "0    65  UO4-2 Commande | Demande de support modificati...  0.428515\n",
              "1    57     UO4-5 Commande | Envoi d'une commande manuelle  0.424080\n",
              "2    30  Message d'erreur \"Renseigner correctement le d...  0.000000\n",
              "3    29  Message d'erreur \"Article ___ non gere dans la...  0.000000\n",
              "4    28  Message d'erreur \"Fonctions oblig. Suivantes n...  0.000000\n",
              "5    27  Message d'erreur \"Qte (+ tolerance livr.excede...  0.000000\n",
              "6    26  Message d'erreur \"Erreur de conversion des qua...  0.000000\n",
              "7    25  Message d'erreur \"Le cpte general ___ ne peut ...  0.000000\n",
              "8    16  Message d'erreur \"Utilisateur ___ traite deja ...  0.000000\n",
              "9    24  Message d'erreur \"Le founisseur XXXX-40 n’exis...  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw06nA5I2jH0"
      },
      "source": [
        "# **3:non-utilisé: Créer les keywords à partir d'une phrase en se basant sur les mots d'un dictionnaire en passant par la tokenization et le removeStopWords**\n",
        "---\n",
        "`BDD: df_news`\n",
        "---\n",
        "*importance: nope*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUmB7tvXmi9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b69780b-b905-408d-ba71-8d0abdedb3e9"
      },
      "source": [
        "#https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb\n",
        "import re\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "with open('stp_words_.txt','r') as f:\n",
        "    Stop_words_strong = f.read()\n",
        "\n",
        "\n",
        "\n",
        "Stop_words_strong = re.findall(r'\\w+',Stop_words_strong)\n",
        "Stop_words_strong.append(\"quelqu\")\n",
        "docs ='''Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment Trouvé apparemment *t sur le parapet de ce mur que s'ouvraient les ailes de la jeunesse. Douce et sainte illusion de l'espoir de la vaincre. Assez avisé pour juger tout effet inutile dans ce cabinet où l'air est si doux, vous ne faites jamais rien sans raison, que c'en était fait du mal. Accoutumé à vivre du présent, nous savons depuis deuxabinet où l'air est si doux, vous ne faites jamais rien sans raison, que c'en était fait du mal. Accoutumé à vivre du présent, nous savons depuis deuxabinet où l'air est si doux, vous ne faites jamais rien sans raison, que c'en était fait du mal. Accoutumé à vivre du présent, nous savons depuis deux jours. Comprenant tout à coup renaissante animait d'une sève nouvelle, sans qu'aucune des précédentes. Abandonnez votre jeu et asseyez-vous à côté de l'avenir de notre entreprise. Planter là, et il reconnut bientôt que c'était impossible de bouger ou de parler. Allez-vous-en, et soyez comme une petite désespérée vers la mule. parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet parapet'''\n",
        "\n",
        "sentence = re.sub('[?,.]','',docs)\n",
        "listes_phrases = sentence.split(' ')\n",
        "cv = CountVectorizer(max_df=0.85,stop_words=Stop_words_strong,max_features=10000)\n",
        "\n",
        "\n",
        "word_count_vector = cv.fit_transform(listes_phrases)\n",
        "\n",
        "#Now, let's look at 10 words from our vocabulary. Sweet, these are mostly programming related.\n",
        "liste = list(cv.vocabulary_.keys())[:]\n",
        "\n",
        "#>>> liste\n",
        "#['trouvé', 'apparemment', 'parapet', 'mur', 'ouvraient', 'ailes', 'jeunesse', 'douce', 'sainte', 'illusion', 'espoir', 'vaincre', 'avisé', 'juger', 'inutile', 'cabinet', 'air', 'doux', 'jamais', 'raison', 'mal', 'accoutumé', 'vivre', 'présent', 'savons', 'jours', 'comprenant', 'coup', 'renaissante', 'animait', 'sève','nouvelle', 'précédentes', 'abandonnez', 'jeu', 'asseyez', 'côté', 'avenir', 'entreprise', 'planter', 'reconnut', 'bientôt', 'impossible', 'bouger', 'parler', 'allez', 'petite', 'désespérée', 'mule']\n",
        "\n",
        "#We can also get the vocabulary by using get_feature_names()\n",
        "liste2 = list(cv.get_feature_names())[2000:2015]\n",
        "\n",
        "# you only needs to do this once\n",
        "feature_names=cv.get_feature_names()\n",
        "print('keywords')\n",
        "feature_names[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keywords\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abandonnez',\n",
              " 'accoutumé',\n",
              " 'ailes',\n",
              " 'air',\n",
              " 'allez',\n",
              " 'animait',\n",
              " 'apparemment',\n",
              " 'asseyez',\n",
              " 'avenir',\n",
              " 'avisé']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}